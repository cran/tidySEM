<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />

<meta name="viewport" content="width=device-width, initial-scale=1" />



<title>Exploratory LPA for Ocean Microplastics</title>

<script>// Pandoc 2.9 adds attributes on both header and div. We remove the former (to
// be compatible with the behavior of Pandoc < 2.8).
document.addEventListener('DOMContentLoaded', function(e) {
  var hs = document.querySelectorAll("div.section[class*='level'] > :first-child");
  var i, h, a;
  for (i = 0; i < hs.length; i++) {
    h = hs[i];
    if (!/^h[1-6]$/i.test(h.tagName)) continue;  // it should be a header h1-h6
    a = h.attributes;
    while (a.length > 0) h.removeAttribute(a[0].name);
  }
});
</script>

<style type="text/css">
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
span.underline{text-decoration: underline;}
div.column{display: inline-block; vertical-align: top; width: 50%;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
</style>



<style type="text/css">
code {
white-space: pre;
}
.sourceCode {
overflow: visible;
}
</style>
<style type="text/css" data-origin="pandoc">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
{ counter-reset: source-line 0; }
pre.numberSource code > span
{ position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
{ content: counter(source-line);
position: relative; left: -1em; text-align: right; vertical-align: baseline;
border: none; display: inline-block;
-webkit-touch-callout: none; -webkit-user-select: none;
-khtml-user-select: none; -moz-user-select: none;
-ms-user-select: none; user-select: none;
padding: 0 4px; width: 4em;
color: #aaaaaa;
}
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa; padding-left: 4px; }
div.sourceCode
{ }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } 
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } 
code span.at { color: #7d9029; } 
code span.bn { color: #40a070; } 
code span.bu { color: #008000; } 
code span.cf { color: #007020; font-weight: bold; } 
code span.ch { color: #4070a0; } 
code span.cn { color: #880000; } 
code span.co { color: #60a0b0; font-style: italic; } 
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } 
code span.do { color: #ba2121; font-style: italic; } 
code span.dt { color: #902000; } 
code span.dv { color: #40a070; } 
code span.er { color: #ff0000; font-weight: bold; } 
code span.ex { } 
code span.fl { color: #40a070; } 
code span.fu { color: #06287e; } 
code span.im { color: #008000; font-weight: bold; } 
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } 
code span.kw { color: #007020; font-weight: bold; } 
code span.op { color: #666666; } 
code span.ot { color: #007020; } 
code span.pp { color: #bc7a00; } 
code span.sc { color: #4070a0; } 
code span.ss { color: #bb6688; } 
code span.st { color: #4070a0; } 
code span.va { color: #19177c; } 
code span.vs { color: #4070a0; } 
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } 
</style>
<script>
// apply pandoc div.sourceCode style to pre.sourceCode instead
(function() {
  var sheets = document.styleSheets;
  for (var i = 0; i < sheets.length; i++) {
    if (sheets[i].ownerNode.dataset["origin"] !== "pandoc") continue;
    try { var rules = sheets[i].cssRules; } catch (e) { continue; }
    var j = 0;
    while (j < rules.length) {
      var rule = rules[j];
      // check if there is a div.sourceCode rule
      if (rule.type !== rule.STYLE_RULE || rule.selectorText !== "div.sourceCode") {
        j++;
        continue;
      }
      var style = rule.style.cssText;
      // check if color or background-color is set
      if (rule.style.color === '' && rule.style.backgroundColor === '') {
        j++;
        continue;
      }
      // replace div.sourceCode by a pre.sourceCode rule
      sheets[i].deleteRule(j);
      sheets[i].insertRule('pre.sourceCode{' + style + '}', j);
    }
  }
})();
</script>




<style type="text/css">body {
background-color: #fff;
margin: 1em auto;
max-width: 700px;
overflow: visible;
padding-left: 2em;
padding-right: 2em;
font-family: "Open Sans", "Helvetica Neue", Helvetica, Arial, sans-serif;
font-size: 14px;
line-height: 1.35;
}
#TOC {
clear: both;
margin: 0 0 10px 10px;
padding: 4px;
width: 400px;
border: 1px solid #CCCCCC;
border-radius: 5px;
background-color: #f6f6f6;
font-size: 13px;
line-height: 1.3;
}
#TOC .toctitle {
font-weight: bold;
font-size: 15px;
margin-left: 5px;
}
#TOC ul {
padding-left: 40px;
margin-left: -1.5em;
margin-top: 5px;
margin-bottom: 5px;
}
#TOC ul ul {
margin-left: -2em;
}
#TOC li {
line-height: 16px;
}
table {
margin: 1em auto;
border-width: 1px;
border-color: #DDDDDD;
border-style: outset;
border-collapse: collapse;
}
table th {
border-width: 2px;
padding: 5px;
border-style: inset;
}
table td {
border-width: 1px;
border-style: inset;
line-height: 18px;
padding: 5px 5px;
}
table, table th, table td {
border-left-style: none;
border-right-style: none;
}
table thead, table tr.even {
background-color: #f7f7f7;
}
p {
margin: 0.5em 0;
}
blockquote {
background-color: #f6f6f6;
padding: 0.25em 0.75em;
}
hr {
border-style: solid;
border: none;
border-top: 1px solid #777;
margin: 28px 0;
}
dl {
margin-left: 0;
}
dl dd {
margin-bottom: 13px;
margin-left: 13px;
}
dl dt {
font-weight: bold;
}
ul {
margin-top: 0;
}
ul li {
list-style: circle outside;
}
ul ul {
margin-bottom: 0;
}
pre, code {
background-color: #f7f7f7;
border-radius: 3px;
color: #333;
white-space: pre-wrap; 
}
pre {
border-radius: 3px;
margin: 5px 0px 10px 0px;
padding: 10px;
}
pre:not([class]) {
background-color: #f7f7f7;
}
code {
font-family: Consolas, Monaco, 'Courier New', monospace;
font-size: 85%;
}
p > code, li > code {
padding: 2px 0px;
}
div.figure {
text-align: center;
}
img {
background-color: #FFFFFF;
padding: 2px;
border: 1px solid #DDDDDD;
border-radius: 3px;
border: 1px solid #CCCCCC;
margin: 0 5px;
}
h1 {
margin-top: 0;
font-size: 35px;
line-height: 40px;
}
h2 {
border-bottom: 4px solid #f7f7f7;
padding-top: 10px;
padding-bottom: 2px;
font-size: 145%;
}
h3 {
border-bottom: 2px solid #f7f7f7;
padding-top: 10px;
font-size: 120%;
}
h4 {
border-bottom: 1px solid #f7f7f7;
margin-left: 8px;
font-size: 105%;
}
h5, h6 {
border-bottom: 1px solid #ccc;
font-size: 105%;
}
a {
color: #0033dd;
text-decoration: none;
}
a:hover {
color: #6666ff; }
a:visited {
color: #800080; }
a:visited:hover {
color: #BB00BB; }
a[href^="http:"] {
text-decoration: underline; }
a[href^="https:"] {
text-decoration: underline; }

code > span.kw { color: #555; font-weight: bold; } 
code > span.dt { color: #902000; } 
code > span.dv { color: #40a070; } 
code > span.bn { color: #d14; } 
code > span.fl { color: #d14; } 
code > span.ch { color: #d14; } 
code > span.st { color: #d14; } 
code > span.co { color: #888888; font-style: italic; } 
code > span.ot { color: #007020; } 
code > span.al { color: #ff0000; font-weight: bold; } 
code > span.fu { color: #900; font-weight: bold; } 
code > span.er { color: #a61717; background-color: #e3d2d2; } 
</style>




</head>

<body>




<h1 class="title toc-ignore">Exploratory LPA for Ocean
Microplastics</h1>



<p>This is an example of exploratory latent class analysis (LCA) with
continuous indicators, otherwise known as latent profile analysis (LPA)
or finite Gaussian mixture modeling, using <code>tidySEM</code>. See Van
Lissa, C. J., Garnier-Villarreal, M., &amp; Anadria, D. (2023).
<em>Recommended Practices in Latent Class Analysis using the Open-Source
R-Package tidySEM.</em> Structural Equation Modeling. <a href="https://doi.org/10.1080/10705511.2023.2250920" class="uri">https://doi.org/10.1080/10705511.2023.2250920</a>. The
present example uses data collected by Alkema as part of a study on
ocean microplastics. The purpose of this study was to provide a more
nuanced model for the distribution of different sizes of ocean
microplastics than the commonly used normal distribution. To this end, a
mixture of normals was used. Since there is no theoretical reason to
expect a certain number of classes, this is an exploratory LCA. To view
its documentation, run the command
<code>?tidySEM::alkema_microplastics</code> in the R console. The
original analyses are available at <a href="https://github.com/cjvanlissa/lise_microplastics" class="uri">https://github.com/cjvanlissa/lise_microplastics</a>; in
this vignette, we take a different approach to the analysis to showcase
other possibilities.</p>
<div id="loading-the-data" class="section level2">
<h2>Loading the Data</h2>
<p>To load the data, simply attach the <code>tidySEM</code> package. For
convenience, we assign the variables used for analysis to an object
called <code>df</code>. As explained in the paper, the classes are quite
different for lines, films, and fragments. For this reason, we here only
use data from fragments. The indicators are fragments’ length and width
in millimeters. The sample size was not planned.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="co"># Load required packages</span></span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a><span class="fu">library</span>(tidySEM)</span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb1-4"><a href="#cb1-4" tabindex="-1"></a><span class="co"># Load data</span></span>
<span id="cb1-5"><a href="#cb1-5" tabindex="-1"></a>df_analyze <span class="ot">&lt;-</span> alkema_microplastics[alkema_microplastics<span class="sc">$</span>category <span class="sc">==</span></span>
<span id="cb1-6"><a href="#cb1-6" tabindex="-1"></a>    <span class="st">&quot;Fragment&quot;</span>, ]</span>
<span id="cb1-7"><a href="#cb1-7" tabindex="-1"></a>df <span class="ot">&lt;-</span> df_analyze[, <span class="fu">c</span>(<span class="st">&quot;length&quot;</span>, <span class="st">&quot;width&quot;</span>)]</span></code></pre></div>
</div>
<div id="descriptive-statistics" class="section level2">
<h2>Descriptive statistics</h2>
<p>As per the best practices, the first step in LCA is examining the
observed data. We use <code>tidySEM::descriptives()</code> to describe
the data numerically. Because all items are continuous, we remove
columns for categorical data to de-clutter the table:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a>desc <span class="ot">&lt;-</span> tidySEM<span class="sc">::</span><span class="fu">descriptives</span>(df)</span>
<span id="cb2-2"><a href="#cb2-2" tabindex="-1"></a>desc <span class="ot">&lt;-</span> desc[, <span class="fu">c</span>(<span class="st">&quot;name&quot;</span>, <span class="st">&quot;type&quot;</span>, <span class="st">&quot;n&quot;</span>, <span class="st">&quot;unique&quot;</span>, <span class="st">&quot;mean&quot;</span>, <span class="st">&quot;median&quot;</span>,</span>
<span id="cb2-3"><a href="#cb2-3" tabindex="-1"></a>    <span class="st">&quot;sd&quot;</span>, <span class="st">&quot;min&quot;</span>, <span class="st">&quot;max&quot;</span>, <span class="st">&quot;skew_2se&quot;</span>, <span class="st">&quot;kurt_2se&quot;</span>)]</span>
<span id="cb2-4"><a href="#cb2-4" tabindex="-1"></a>knitr<span class="sc">::</span><span class="fu">kable</span>(desc, <span class="at">caption =</span> <span class="st">&quot;Descriptive statistics&quot;</span>)</span></code></pre></div>
<p>The data are correctly coded as <code>numeric</code> and the
distributional characteristics match the intended measurement level. The
variable scales are comparable (both in millimeters and no large
discrepancies between variances). There are no missing values; if any
variables had missing values, we would report an MCAR test with
<code>mice::mcar()</code>, and explain that missing data are accounted
for using FIML. Additionally, we can plot the data. The
<code>ggplot2</code> function <code>geom_density()</code> is useful to
visualize continuous data:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a>df_plot <span class="ot">&lt;-</span> df</span>
<span id="cb3-2"><a href="#cb3-2" tabindex="-1"></a><span class="fu">names</span>(df_plot) <span class="ot">&lt;-</span> <span class="fu">paste0</span>(<span class="st">&quot;Value.&quot;</span>, <span class="fu">names</span>(df_plot))</span>
<span id="cb3-3"><a href="#cb3-3" tabindex="-1"></a>df_plot <span class="ot">&lt;-</span> <span class="fu">reshape</span>(df_plot, <span class="at">varying =</span> <span class="fu">names</span>(df_plot), <span class="at">direction =</span> <span class="st">&quot;long&quot;</span>,</span>
<span id="cb3-4"><a href="#cb3-4" tabindex="-1"></a>    <span class="at">timevar =</span> <span class="st">&quot;Variable&quot;</span>)</span>
<span id="cb3-5"><a href="#cb3-5" tabindex="-1"></a><span class="fu">ggplot</span>(df_plot, <span class="fu">aes</span>(<span class="at">x =</span> Value)) <span class="sc">+</span> <span class="fu">geom_density</span>() <span class="sc">+</span> <span class="fu">facet_wrap</span>(<span class="sc">~</span>Variable) <span class="sc">+</span></span>
<span id="cb3-6"><a href="#cb3-6" tabindex="-1"></a>    <span class="fu">theme_bw</span>()</span></code></pre></div>
<p>Both the table above and the density plot indicate that the data are
extremely right-skewed and kurtotic. With this in mind, it can be useful
to transform and rescale the data. We will use a log transformation.</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" tabindex="-1"></a>df_plot<span class="sc">$</span>Value <span class="ot">&lt;-</span> <span class="fu">log</span>(df_plot<span class="sc">$</span>Value)</span>
<span id="cb4-2"><a href="#cb4-2" tabindex="-1"></a><span class="fu">ggplot</span>(df_plot, <span class="fu">aes</span>(<span class="at">x =</span> Value)) <span class="sc">+</span> <span class="fu">geom_density</span>() <span class="sc">+</span> <span class="fu">facet_wrap</span>(<span class="sc">~</span>Variable) <span class="sc">+</span></span>
<span id="cb4-3"><a href="#cb4-3" tabindex="-1"></a>    <span class="fu">theme_bw</span>()</span></code></pre></div>
<p>The log transformation addresses the aforementioned concerns
regarding skew and kurtosis. To confirm this, reshape the data to wide
format and examine a scatterplot:</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" tabindex="-1"></a>df <span class="ot">&lt;-</span> <span class="fu">reshape</span>(df_plot, <span class="at">direction =</span> <span class="st">&quot;wide&quot;</span>, <span class="at">v.names =</span> <span class="st">&quot;Value&quot;</span>)[,</span>
<span id="cb5-2"><a href="#cb5-2" tabindex="-1"></a>    <span class="sc">-</span><span class="dv">1</span>]</span>
<span id="cb5-3"><a href="#cb5-3" tabindex="-1"></a><span class="fu">names</span>(df) <span class="ot">&lt;-</span> <span class="fu">gsub</span>(<span class="st">&quot;Value.&quot;</span>, <span class="st">&quot;&quot;</span>, <span class="fu">names</span>(df), <span class="at">fixed =</span> <span class="cn">TRUE</span>)</span>
<span id="cb5-4"><a href="#cb5-4" tabindex="-1"></a><span class="fu">ggplot</span>(df, <span class="fu">aes</span>(<span class="at">x =</span> length, <span class="at">y =</span> width)) <span class="sc">+</span> <span class="fu">geom_point</span>(<span class="at">alpha =</span> <span class="fl">0.1</span>) <span class="sc">+</span></span>
<span id="cb5-5"><a href="#cb5-5" tabindex="-1"></a>    <span class="fu">theme_bw</span>()</span></code></pre></div>
<!-- As evident from the scatterplot, there is a very strong linear tend in the data. -->
<!-- This indicates that fragments tend to be coextensive (when length goes up, width goes up). -->
<!-- We can analyze these data in their original dimensions (length and width). -->
<!-- Alternatively, we can use PCA to rotate the data such that the first dimension can be interpreted as "size", -->
<!-- and the second dimension can be interpreted as "shape": -->
</div>
<div id="conducting-latent-profile-analysis" class="section level2">
<h2>Conducting Latent Profile Analysis</h2>
<p>As all variables are continuous, we can use the convenience function
<code>tidySEM::mx_profiles()</code>, which is a wrapper for the generic
function <code>mx_mixture()</code> optimized for continuous indicators.
Its default settings are appropriate for LPA, assuming fixed variances
across classes and zero covariances. Its arguments are <code>data</code>
and number of <code>classes</code>. All variables in <code>data</code>
are included in the analysis, which is why we first selected the
indicator variables. The models are estimated using simulated annealing,
with start values determined via initial K-means clustering.</p>
<p>As this is an exploratory LCA, we will conduct a rather extensive
search across model specifications and number of classes. We will set
the maximum number of classes <span class="math inline">\(K\)</span> to
three to limit computational demands. We set a seed to ensure replicable
results.</p>
<p>As the analysis takes a long time to compute, it is prudent to save
the results to disk immediately, so as not to lose them. For this, we
use the function <code>saveRDS()</code>. We can later use
<code>res &lt;- readRDS(&quot;res_gmm.RData&quot;)</code> to load the analysis
from the file.</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb6-2"><a href="#cb6-2" tabindex="-1"></a>res <span class="ot">&lt;-</span> <span class="fu">mx_profiles</span>(<span class="at">data =</span> df, <span class="at">classes =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">3</span>, <span class="at">variances =</span> <span class="fu">c</span>(<span class="st">&quot;equal&quot;</span>,</span>
<span id="cb6-3"><a href="#cb6-3" tabindex="-1"></a>    <span class="st">&quot;varying&quot;</span>), <span class="at">covariances =</span> <span class="fu">c</span>(<span class="st">&quot;equal&quot;</span>, <span class="st">&quot;varying&quot;</span>), <span class="at">expand_grid =</span> <span class="cn">TRUE</span>)</span>
<span id="cb6-4"><a href="#cb6-4" tabindex="-1"></a><span class="fu">saveRDS</span>(res, <span class="st">&quot;res_gmm.RData&quot;</span>)</span></code></pre></div>
<!-- Note that one model (the most complex one) returns the following warning: -->
<!-- > Warning: In model 'mix4' Optimizer returned a non-zero status code 6. The model does not satisfy the first-order optimality conditions to the required accuracy, and no improved point for the merit function could be found during the final linesearch (Mx status RED) -->
<!-- This suggests a potential problem with with model convergence. -->
<!-- To try and get this model to converge, it is possible to use the function `mxTryHard()`, which permutes the starting values and tries to find a better solution. -->
<!-- To do this, run `res[[24]] <- mxTryHard(res[[24]])`. -->
<!-- However, at present, we will skip this step, -->
<!-- because there are also other indications that this 4-class model is overfitted (see below). -->
</div>
<div id="class-enumeration" class="section level2">
<h2>Class Enumeration</h2>
<p>To compare the fit of the estimated models, we create a model fit
table using <code>table_fit()</code>. We will use the BIC for class
enumeration.</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" tabindex="-1"></a>fit <span class="ot">&lt;-</span> <span class="fu">table_fit</span>(res)</span></code></pre></div>
<p>First, we determine whether any models can be disqualified. There
were no indications of convergence problems during estimation, so this
is not a reason to disqualify solutions. Next, we check for global and
local identifiability. The global ratio of observations per parameter is
large, as the minimum <code>np_ratio</code> is 244. The smallest ratio
of class size to class-specific parameters is 18 (see
<code>np_local</code>), which is no cause for concern.</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" tabindex="-1"></a>tbl <span class="ot">&lt;-</span> fit[, <span class="fu">c</span>(<span class="st">&quot;Name&quot;</span>, <span class="st">&quot;LL&quot;</span>, <span class="st">&quot;Parameters&quot;</span>, <span class="st">&quot;BIC&quot;</span>, <span class="st">&quot;Entropy&quot;</span>,</span>
<span id="cb8-2"><a href="#cb8-2" tabindex="-1"></a>    <span class="st">&quot;prob_min&quot;</span>, <span class="st">&quot;n_min&quot;</span>, <span class="st">&quot;np_ratio&quot;</span>, <span class="st">&quot;np_local&quot;</span>)]</span>
<span id="cb8-3"><a href="#cb8-3" tabindex="-1"></a><span class="fu">names</span>(tbl) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;Name&quot;</span>, <span class="st">&quot;LL&quot;</span>, <span class="st">&quot;p&quot;</span>, <span class="st">&quot;BIC&quot;</span>, <span class="st">&quot;Ent.&quot;</span>, <span class="st">&quot;p_min&quot;</span>, <span class="st">&quot;n_min&quot;</span>,</span>
<span id="cb8-4"><a href="#cb8-4" tabindex="-1"></a>    <span class="st">&quot;np_ratio&quot;</span>, <span class="st">&quot;np_local&quot;</span>)</span>
<span id="cb8-5"><a href="#cb8-5" tabindex="-1"></a>knitr<span class="sc">::</span><span class="fu">kable</span>(tbl, <span class="at">caption =</span> <span class="st">&quot;Model fit table.&quot;</span>)</span></code></pre></div>
<p>However, note that we have a very large sample, and for many models,
the smallest class comprises only a very small percentage of the total
sample. Since the purpose of this analysis is to better represent the
distribution of ocean microplastics, we can wonder whether it makes
sense to allow for classes that only describe a small percentage of the
cases. We therefore only consider solutions that capture at least 10% of
the sample.</p>
<p>Another interesting characteristic of this data is that the BIC and
the entropy are strongly correlated. The raw correlation between these
two metrics is .66, <code>cor(fit$BIC, fit$Entropy)</code>. If we omit
the 1-class models, for which entropy is technically not defined, the
correlation is even as high as .85,
<code>cor(fit$BIC[!fit$Classes == 1], fit$Entropy[!fit$Classes == 1])</code>.</p>
<p>This strong correlation indicates that an increase in fit comes with
a decrease in class separability. This illustrates why entropy should
not be treated as a model fit criterion. It also illustrates that
criteria for class enumeration should be explicit, because we will
likely come to a different decision depending on which criteria are
used.</p>
<p>As mentioned before, we drop models with &lt; 10% of cases in the
smallest class:</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" tabindex="-1"></a>fit <span class="ot">&lt;-</span> fit[<span class="sc">!</span>fit<span class="sc">$</span>n_min <span class="sc">&lt;</span> <span class="fl">0.1</span>, ]</span></code></pre></div>
<p>If our strategy is to optimize fit, we can examine the fit table
above, or plot a scree plot for the BIC by calling
<code>plot(fit)</code>. Note that, due to the large sample size, all ICs
give identical conclusions.</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" tabindex="-1"></a><span class="fu">plot</span>(fit) <span class="sc">+</span> <span class="fu">theme</span>(<span class="at">axis.text.x =</span> <span class="fu">element_text</span>(<span class="at">angle =</span> <span class="dv">90</span>, <span class="at">vjust =</span> <span class="fl">0.5</span>,</span>
<span id="cb10-2"><a href="#cb10-2" tabindex="-1"></a>    <span class="at">hjust =</span> <span class="dv">1</span>))</span></code></pre></div>
<p>Looking at the blocks of 1-4 class models for each model
specification, it appears that the BIC keeps decreasing with the
addition of more classes. Across the blocks, the BIC keeps decreasing
with increasingly complex model specifications.</p>
<p>Out of the 16 models that remain after removing those with &lt; 10%
of cases in the smallest class, one model stands out: The 2-class model
with free (co)variances.
<!-- The function `ic_weights(fit)` allows us to compute IC weights for all models in the set; -->
<!-- it prefers the most complex model out of the 16 models with a posterior model probability of nearly 100%. -->
We thus select this as our final model.</p>
</div>
<div id="interpreting-the-final-class-solution" class="section level2">
<h2>Interpreting the Final Class Solution</h2>
<p>We here request the estimates (<code>est</code>) and standardized
estimates <code>std_est</code>, because the latter allows us to
interpret the correlations between length and width. Note that standard
errors and p-values are relatively uninformative: With a sample size of
5606, every parameter is significantly different from zero.</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" tabindex="-1"></a>res_bic <span class="ot">&lt;-</span> res[[<span class="st">&quot;free var, free cov 2&quot;</span>]]</span>
<span id="cb11-2"><a href="#cb11-2" tabindex="-1"></a>cp <span class="ot">&lt;-</span> <span class="fu">class_prob</span>(res_bic)</span>
<span id="cb11-3"><a href="#cb11-3" tabindex="-1"></a>results <span class="ot">&lt;-</span> <span class="fu">table_results</span>(res_bic, <span class="at">columns =</span> <span class="fu">c</span>(<span class="st">&quot;label&quot;</span>, <span class="st">&quot;est&quot;</span>,</span>
<span id="cb11-4"><a href="#cb11-4" tabindex="-1"></a>    <span class="st">&quot;std_est&quot;</span>))</span>
<span id="cb11-5"><a href="#cb11-5" tabindex="-1"></a>results</span></code></pre></div>
<p>Interpreting the results is facilitated by examining a plot of the
model and data. Relevant plot functions are
<code>plot_bivariate()</code>, <code>plot_density()</code>, and
<code>plot_profiles()</code>. However, we omit the density plots,
because <code>plot_bivariate()</code> also includes them.</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1" tabindex="-1"></a><span class="fu">plot_bivariate</span>(res_bic)</span></code></pre></div>
<p>On the diagonal of the bivariate plot are weighted density plots:
normal approximations of the density function of observed data, weighed
by class probability. On the off-diagonal are plots for each pair of
indicators, with the class means indicated by a point, class standard
deviations indicated by lines, and covariances indicated by circles.</p>
<p>The bivariate and marginal plots show that the classes are not
clearly separable, as also evident from the low entropy. At the same
time however, it is clear that the observed distributions are
non-normal, and the second class accounts for some of this non-normality
(there is a smaller ‘bump’ to the right of the mode, which could be the
mean of a second normal distribution). The first class (57%) accounts
for smaller fragments, and the second class (43%) accounts for some of
the right-skew in fragments’ length and width. We label class 1 as
<em>small fragments</em>, and class 2 as <em>larger fragments</em>.</p>
<p>It also appears that the correlation between length and width is
stronger for small fragments than for large fragments. To test the
difference, use
<code>wald_test(res_bic, hypothesis = &quot;c11 = c21&quot;)</code>. Results
indicate that the correlation is indeed significantly larger for small
fragments (<span class="math inline">\(r = .92\)</span>) than for larger
fragments (<span class="math inline">\(r = .85\)</span>), <span class="math inline">\(\chi^2(1) = 11.56, p &lt; .001\)</span>. Thus,
small fragments are more coextensive than large fragments.</p>
<p>There are, however, concerns about the interpretability of this
solutions: the entropy is <code>.56</code> and the minimum
classification probability is<code>.81</code>. This is because of
substantial overlap in the distributions of the two classes.</p>
</div>
<div id="auxiliary-analyses" class="section level2">
<h2>Auxiliary Analyses</h2>
<p>Finally, we may want to compare the different classes on auxiliary
variables or models. The <code>BCH()</code> function applies three-step
analysis, which compares the classes using a multi-group model,
controlling for classification error. For example, we can test whether
polymer type differs between the two classes. Because polymer type is a
nominal variable, we must convert it to dummies and estimate a threshold
for each dummy:</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" tabindex="-1"></a>df_pt <span class="ot">&lt;-</span> <span class="fu">mx_dummies</span>(df_analyze<span class="sc">$</span>poly_type)</span>
<span id="cb13-2"><a href="#cb13-2" tabindex="-1"></a>aux_pt <span class="ot">&lt;-</span> <span class="fu">BCH</span>(res_bic, <span class="at">model =</span> <span class="st">&quot;poly_typeOther | t1</span></span>
<span id="cb13-3"><a href="#cb13-3" tabindex="-1"></a><span class="st">                                poly_typePE | t1</span></span>
<span id="cb13-4"><a href="#cb13-4" tabindex="-1"></a><span class="st">                                poly_typePP | t1&quot;</span>,</span>
<span id="cb13-5"><a href="#cb13-5" tabindex="-1"></a>    <span class="at">data =</span> df_pt)</span>
<span id="cb13-6"><a href="#cb13-6" tabindex="-1"></a>aux_pt <span class="ot">&lt;-</span> <span class="fu">mxTryHardOrdinal</span>(aux_pt)</span></code></pre></div>
<p>To obtain an omnibus likelihood ratio test of the significance of the
differences in polymer type across classes, use
<code>lr_test(aux_pt)</code>. The results indicate that there are
significant differences in polymer types across classes, <span class="math inline">\(\Delta LL(3) = 17.14, p &lt; .001\)</span>. The
results can be reported in probability scale using
<code>table_prob(aux_pt)</code>. To test differences for specific
polymer types, we can use Wald tests:</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1" tabindex="-1"></a><span class="fu">wald_test</span>(aux_pt, <span class="st">&quot;class1.Thresholds[1,1] = class2.Thresholds[1,1];</span></span>
<span id="cb14-2"><a href="#cb14-2" tabindex="-1"></a><span class="st">          class1.Thresholds[1,2] = class2.Thresholds[1,2];</span></span>
<span id="cb14-3"><a href="#cb14-3" tabindex="-1"></a><span class="st">          class1.Thresholds[1,3] = class2.Thresholds[1,3]&quot;</span>)</span></code></pre></div>
<p>The results indicate that there is no significant difference in the
prevalence of “Other” polymer types across classes. However, PE is
significantly more prevalent in class 1, and PP is significantly more
prevalent in class 2.</p>
</div>



<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
